[
{
  "text": "Call yourself an AI PM, AI Engineer, AI Consultant, or AI Leadâ€”but stick around long enough, and youâ€™ll realize one thing: youâ€™re a problem solver. And the real problem is not AI, itâ€™s data.\n\nYouâ€™ll spend most of your time wrangling messy real-world data rather than playing with the latest shiny AI tool.\n\nSo when someone tells me about 10 new advanced RAG methods theyâ€™ve learned or a fancy multi-agent system theyâ€™re excited about, my first question is: Does it actually work for your problem, your data, your business? How do you plan to evaluate it?\n\nFunny enough, but if you're always chasing the latest trends, youâ€™ll never actually catch up, because you donâ€™t know why, where, or when they should be used.",
  "engagement": 0
},
{
  "text": "This is really interesting! A group of folks have launched a YouTube series called 'Building DeepSeek from Scratch' to teach how you can create your own mini-DeepSeek.\n\nRaj Abhijit and team are putting together a series of 15-minute videos, covering 40+ hours of content on how to build DeepSeek-like reasoning models.\n\nKey topics covered:\n1. Mixture of Experts (MoE)\n2. Multi-head Latent Attention (MLA)\n3. Rotary Positional Encodings (RoPE)\n4. Multi-token Prediction (MTP)\n5. Supervised Fine-Tuning (SFT)\n6. Group Relative Policy Optimization (GRPO)\n\nIn Rajâ€™s words:\n'After this, youâ€™ll be among the top 0.1% of ML/LLM engineers who can build DeepSeekâ€™s core ingredients on their own.'\n\nI watched the first video, and it looks promising. While I believe things can get exponentially more complex at scale, where DeepSeek-R1â€™s real strength lies, building a small version to understand the core concepts seems like a valuable learning experience!",
  "engagement": 0
},
{
  "text": "To everyone busy posting about Perplexityâ€™s 'Deep Research' as a replacement for OpenAI's, have you actually used it?\n\nIt hallucinates like crazy. ğŸ¤¯\n\nTake a look at this simple example: https://lnkd.in/e77TA_4y\n\nMany of the claims it generates are either linked to the wrong source or simply donâ€™t exist!\n\nğŸ˜… None of the below claims it made can be verified (just a few examples, cmd+f and try looking for the numbers or claims)\n\n1. 'Generative AI now automates 20-45% of software engineering workflows through tools like Codeium's context-aware code generation'\n2. 'Expected 2026 EU AI Act compliance requirements driving $12B investment in governance tools'\n3. '45% faster product iteration cycles in manufacturing'\n\nğŸ˜… I couldnâ€™t find any trace of many claims or numbers in the cited articles, these are just few examples from the report, I'm sure you'll find many more.\n\nNot supporting OpenAI or anything, but their deep research is way better. Iâ€™d rather pay $200 than wade through a ton of hallucinations and fact-check everything myself.",
  "engagement": 0
},
{
  "text": "This repo is super impressive, it covers the entire end-to-end lifecycle of AI models, from data processing to training, evaluation and even deployment!\n\nOumi is one of the most comprehensive open-source platforms Iâ€™ve come across, genuinely democratizing model building for everyone.\n\nKey features from the repo:\nâ›³ Train and fine-tune models from 10M to 405B parameters using techniques like SFT, LoRA, QLoRA, and DPO\nâ›³ Supports both text and multimodal models, including Llama, DeepSeek, Qwen, and Phi\nâ›³ Synthesize and curate training data with LLM judges\nâ›³ Deploy efficiently with inference engines like vLLM and SGLang\nâ›³ Evaluate models across standard benchmarks\nâ›³ Runs on anythingâ€”laptops, clusters, or cloud (AWS, Azure, GCP, Lambda, etc.)\nâ›³ Integrates with both open models and commercial APIs like OpenAI, Anthropic, and Vertex AI\n\nIt also includes starter notebooks for fine-tuning, distillation, LLM judges, and more. Itâ€™s well thought out and already has â­ 6.5k stars on GitHub.\n\nKnowing Emmanouil (Manos) Koukoumidis and his passion for AI, I have no doubt this will become a go-to platform for open-source model building and deployment.\nHuge congrats, this is going to be big! ğŸš€",
  "engagement": 347
},
  {
  "text": "Kafka, Hadoop, and Spark â€“ Which One Should You Use? âš™ï¸ğŸ“Š\nWhen handling large-scale data, choosing the right technology can be challenging. Each tool offers unique strengths, and picking the right one depends on your data needs. Here's a simple breakdown:\n\nğŸ”¹ Apache Kafka â€“ Best for Real-Time Data Streaming\nâœ… Handles event-driven architectures and real-time data pipelines.\nâœ… Ideal for tracking live data feeds, like social media streams or financial transactions.\nğŸ“Œ Example Use Case: Processing 100,000+ tweets in real-time for sentiment analysis.\n\nğŸ”¹ Apache Hadoop â€“ Best for Batch Processing of Large Datasets\nâœ… Efficient for storing and processing massive datasets using HDFS (Hadoop Distributed File System).\nâœ… Ideal for historical data analysis and ETL pipelines.\nğŸ“Œ Example Use Case: Analyzing customer behavior data over the past 5 years.\n\nğŸ”¹ Apache Spark â€“ Best for Fast Distributed Data Processing\nâœ… Faster than Hadoop for processing large datasets in memory.\nâœ… Supports batch, streaming, and machine learning workloads.\nğŸ“Œ Example Use Case: Building predictive models with millions of records.\n\nğŸš€ Which One Should You Use?\nğŸ”¸ Use Kafka for real-time event streaming.\nğŸ”¸ Use Hadoop for cost-effective batch processing of historical data.\nğŸ”¸ Use Spark for fast, scalable data analytics with in-memory computing.\n\nğŸ’¡ Pro Tip: Combining these tools can unlock powerful data pipelines, (Kafka for data ingestion, Spark for real-time processing, and Hadoop for long-term data storage).",
  "engagement": 545
}
]